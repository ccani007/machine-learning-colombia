---
title: "Técnicas de Regularización: Ridge, Lasso y Elastic Net usando `tidymodels`"
author: "Catalina Cañizares, Ph.D. y Francisco Cardozo Ph.D."
format:
  revealjs:
    theme: simple
    css: lasso-css.css
    scrollable: true
    slide-number: true
    transition: "fade"
    auto-play-media: true
    auto-stretch: true
editor_options: 
  chunk_output_type: console
---

## Acerca de Este Material

> Técnicas de Regularización: Ridge, Lasso y Elastic Net usando `tidymodels` © 2024 por Catalina Canizares y Francisco Cardozo está licenciado bajo Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International 

Este material está disponible libremente bajo la Licencia Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International. 

Para más información sobre esta licencia, por favor visite: [Licencia Creative Commons](https://creativecommons.org/licenses/by-nc-nd/4.0/)



## Reducción (Shrinkage)

```{r}
library(tidyverse)
library(tidymodels)
library(skimr)
```

- La reducción es una técnica que "reduce" o disminuye los coeficientes de las variables predictoras a cero.
- Los métodos de reducción logran un equilibrio entre capturar las relaciones en los datos y evitar el sobreajuste.
- Reducir las estimaciones de los coeficientes puede disminuir significativamente la varianza.
- Las dos técnicas más conocidas para reducir los coeficientes de regresión hacia cero son la regresión ridge y el lasso.

:::footer
[ISLRv2](https://hastie.su.domains/ISLR2/ISLRv2_website.pdf)
:::

## Suma Residual de Cuadrados (RSS)

:::: {.columns}
::: {.column width="50%"}

El RSS mide la cantidad de error que queda entre la función de regresión y el conjunto de datos después de ejecutar el modelo.   
Un RSS más pequeño representa una función de regresión que se ajusta bien a los datos.


$$
\text{RSS} = \sum_{i=1}^n\bigg( y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij}\bigg)^2
$$


:::
::: {.column width="50%"}

![](img/rss.png)

:::
:::
:::footer
[ISLRv2](https://hastie.su.domains/ISLR2/ISLRv2_website.pdf)
:::

# Regresión Ridge

## Ridge
- La regresión ridge es una técnica de regularización que agrega un término de penalización ($\lambda$) al RSS.

- Reduce los coeficientes de regresión hacia cero, disminuyendo su impacto en el modelo.

- También se conoce como regularización L2

- La fórmula para la regresión Ridge es:
$$
\sum_{i=1}^n\bigg( y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij}\bigg)^2 + \lambda \sum_{j=1}^p \beta_j^2 
$$

:::footer
[ISLRv2](https://hastie.su.domains/ISLR2/ISLRv2_website.pdf)
:::

## Ridge 

:::: {.columns}
::: {.column width="50%"}
![](img/ridge_graph.png)

:::
::: {.column width="50%"}
Cuando $\lambda$ es extremadamente grande, todas las estimaciones de los coeficientes ridge
son básicamente cero; esto corresponde al modelo nulo que no contiene predictores. 
:::
:::

:::footer
[ISLRv2](https://hastie.su.domains/ISLR2/ISLRv2_website.pdf)
:::

# Lasso

## Lasso

- Lasso (Operador de Reducción y Selección de Mínimos Absolutos) es otra técnica de regularización que agrega un término de penalización ($\lambda$) al RSS

- Tiene una propiedad incorporada de selección de características, ya que puede reducir los coeficientes exactamente a cero.

- También se conoce como regularización L1


$$
\sum_{i=1}^n\bigg( y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij}\bigg)^2 + \lambda \sum_{j=1}^p | \beta_j| 
$$

:::footer
[ISLRv2](https://hastie.su.domains/ISLR2/ISLRv2_website.pdf)
:::

## Lasso

:::: {.columns}
::: {.column width="50%"}
![](img/lasso_graph.png)

:::
::: {.column width="50%"}
Cuando λ = 0, el lasso simplemente da el ajuste de mínimos cuadrados, y cuando $\lambda$ se vuelve suficientemente
grande, el lasso da el modelo nulo en el que todas las estimaciones de coeficientes son iguales
a cero. 
:::
:::

:::footer
[ISLRv2](https://hastie.su.domains/ISLR2/ISLRv2_website.pdf)
:::

## Comparar Ridge vs. Lasso

:::: {.columns}
::: {.column width="50%"}
![](img/lasso_graph.png)

Dependiendo del valor de $\lambda$, el lasso puede producir un modelo que involucre cualquier número de variables.

:::
::: {.column width="50%"}
![](img/ridge_graph.png)
Ridge siempre incluirá todas las variables en el modelo, aunque la magnitud de las estimaciones de los coeficientes dependerá de $\lambda$
:::
:::

:::footer
[ISLRv2](https://hastie.su.domains/ISLR2/ISLRv2_website.pdf)
:::

## La Propiedad de Selección de Variables del Lasso

![](img/Contours .png)

-El hecho de que algunos coeficientes del lasso se reduzcan completamente a cero explica por qué el lasso realiza selección de características

::: {.notes}
La estimación de mínimos cuadrados está marcada como beta sombrero,
Las regiones restrictivas están en gris, la región de diamante es para el lasso, la región de círculo es para el ridge.
Las elipses centradas alrededor de βˆ representan un contorno de RSS
A medida que las elipses se expanden, el RSS aumenta.
Las estimaciones de lasso y ridge son los primeros puntos en los que una elipse contrae la región gris.
:::

:::footer
[ISLRv2](https://hastie.su.domains/ISLR2/ISLRv2_website.pdf)
:::

## Elastic Net

- Es una generalización de la regresión lasso y ridge
- Combina las dos penalizaciones. 
- La fórmula para Elastic Net:

$$
\sum_{i=1}^n\bigg( y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij}\bigg)^2 + \lambda \sum_{j=1}^p \beta_j^2  + \lambda \sum_{j=1}^p | \beta_j| 
$$

:::footer
[ISLRv2](https://hastie.su.domains/ISLR2/ISLRv2_website.pdf)
:::

## Elastic Net

La ventaja del elastic net es que **mantiene la calidad de selección de características** de la penalización lasso así como la **efectividad de la penalización ridge**. 

:::footer
[Practitioner’s Guide to Data Science](https://scientistcafe.com/ids/elastic-net.html)
:::

## ¿Cómo elegimos el mejor $\lambda$?

1. Usando métodos de remuestreo
 - Verificar el rendimiento usando RMSE o ROC-AUC
2. Podemos verificar desde sin reducción hasta tanto que no nos queden predictores (lasso)

# Lasso con `tidymodels`
![](img/woody_amazed.gif)

## Tarea 

:::: {.columns}
::: {.column width="50%"}

Determinar los predictores clave de un conjunto integral de 42 comportamientos de riesgo, que incluyen el consumo de alcohol y drogas, así como actividades imprudentes, con el objetivo principal de predecir con precisión la probabilidad de participar en **enviar mensajes de texto y conducir**. 

:::
::: {.column width="50%"}
::: {.fragment}

![](https://media.giphy.com/media/eGyJz0UnrEy3K/giphy.gif?cid=ecf05e470l48hi14w34gpqwhaddfljk9xuwn2xir0rn52323&ep=v1_gifs_search&rid=giphy.gif&ct=g)

:::
:::
:::

## Los Datos - `riskyBehaviors`

```{r}
#| echo: true
library(MLearnYRBSS)
data("riskyBehaviors")

```

## Limpieza de Datos
```{r}
#| echo: true
#| code-line-numbers: "1|2|3|4|5|6|7|8|9|10|11|12"
riskyBehaviors_analysis <-
    riskyBehaviors |>
    mutate(
        TextingDriving = case_when(
            TextingDriving == 1 ~ 0,
            TextingDriving %in% c(2, 3, 4, 5) ~ 1,
            TRUE ~ NA
        )
    ) |>
    mutate(TextingDriving = factor(TextingDriving)) |>
    drop_na(TextingDriving) |>
    select(
        -SourceVaping,
        -SourceAlcohol,
        -SexOrientation,
        -DrivingDrinking,
        -SexualAbuse,
        -SexualAbuseByPartner,
        -Grade,
        -Age
    )

```


## División Entrenamiento - Prueba 

```{r}
#| echo: true
#| code-line-numbers: "1|3|4|6|7|9"
#| output-location: fragment

set.seed(1990)

analysis_split <- initial_split(
    riskyBehaviors_analysis,
    strata = TextingDriving
)

analysis_train <- training(analysis_split)
analysis_test <- testing(analysis_split)

analysis_split

```

## Verifiquemos Nuestro Trabajo

```{r}
library(janitor)
```


:::: {.columns}
::: {.column width="50%"}

```{r}
#| echo: true
#| code-line-numbers: "1|2|3|4"
#| output-location: fragment

analysis_train |>
    tabyl(TextingDriving) |>
    adorn_pct_formatting(0) |>
    adorn_totals()


```

:::

::: {.column width="50%"}

```{r}
#| echo: true
#| code-line-numbers: "1|2|3|4"
#| output-location: fragment

analysis_test |>
    tabyl(TextingDriving) |>
    adorn_pct_formatting(0) |>
    adorn_totals()
```

:::
:::


## Validación Cruzada 

```{r}
#| echo: true
#| code-line-numbers: "1|3|5"
#| output-location: fragment

set.seed(1990)

analysis_folds <- vfold_cv(analysis_train, v = 5)
analysis_folds
```


## Receta

```{r}
#| echo: true
#| code-line-numbers: "1|2|3|4|5|6|7|9"
#| output-location: fragment

texting_rec <-
    recipe(TextingDriving ~ ., data = analysis_train) |>
    step_zv(all_predictors()) |>
    step_impute_mode(all_nominal_predictors()) |>
    step_impute_mean(all_numeric_predictors()) |>
    step_normalize(all_numeric_predictors()) |>
    step_dummy(all_nominal_predictors())

texting_rec

```

## Especificación


```{r}
#| echo: true
#| code-line-numbers: "1|2|3|4|5|6|7|8|10"
#| output-location: fragment

texting_spec <-
    logistic_reg(penalty = tune(), mixture = 1) |>
    # mixture = 1 especifica un modelo lasso puro,
    # mixture = 0 especifica un modelo de regresión ridge, y
    # 0 < mixture < 1 especifica un modelo elastic net, interpolando lasso y ridge.
    # https://parsnip.tidymodels.org/reference/glmnet-details.html
    set_engine('glmnet')

texting_spec

```



## Flujo de Trabajo

```{r}
#| echo: true
#| code-line-numbers: "1|2|3|4|6"
#| output-location: fragment
texting_wf <-
    workflow() |>
    add_recipe(texting_rec) |>
    add_model(texting_spec)

texting_wf
```


## La Cuadrícula


:::{.columns}
:::{.column width="50%"} 

- Una búsqueda en cuadrícula explora sistemáticamente diferentes valores de penalización (por ejemplo, 0.001, 0.01, 0.1, 1, 10).
- El rendimiento del modelo se evalúa para cada combinación de valores de penalización.
:::

:::{.column width="50%"}
![](img/grid.gif)
:::
:::


## La Cuadrícula
![](https://dials.tidymodels.org/logo.png){.absolute top=0 right=0 width=80 height=90}

::: panel-tabset 

### 1

```{r}
#| echo: true
#| code-line-numbers: "1|2|3"
#| output-location: fragment
# Usado para la clase del Dr. Balise
glmnet_grid <- data.frame(penalty = 10^seq(-6, -1, length.out = 20))
glmnet_grid

```

### 2

```{r}
#| echo: true
#| code-line-numbers: "1|2|3"
#| output-location: fragment
# Lo inventé
penalty_grid <- grid_regular(penalty(range = c(-4, 4)), levels = 50)
penalty_grid
```

### 3

```{r}
#| echo: true
#| code-line-numbers: "1|2|3"
#| output-location: fragment
# Lo que recomendaría
lambda_grid <- grid_regular(penalty(), levels = 50)
lambda_grid
```

:::

## ¿Cómo determinar la cuadrícula?

![](img/grid_search_3.png){.absolute}

:::footer
[Link to Article](https://pubmed.ncbi.nlm.nih.gov/20808728/)
:::

## ¿Cómo determinar la cuadrícula?

![](img/grid_search_2.png){.absolute}

:::footer
[ISLR tidymodels labs](https://emilhvitfeldt.github.io/ISLR-tidymodels-labs/06-regularization.html)
:::

## Ajustar la Cuadrícula Usando el Flujo de Trabajo 
![](https://tune.tidymodels.org/logo.png){.absolute top=0 right=0 width=80 height=90}
<br>
<br>
```{r}
#| echo: true
#| code-line-numbers: "1|3|5|6|7|8|9|10|11|12"
#| output-location: fragment

doParallel::registerDoParallel()

set.seed(2023)

lasso_tune <- 
  tune_grid(
  object = texting_wf, 
  resamples = analysis_folds,
  grid = lambda_grid, 
  control = control_resamples(event_level = "second")
)

doParallel::stopImplicitCluster()
```

## Recolectar las Métricas
![](https://tune.tidymodels.org/logo.png){.absolute top=0 right=0 width=80 height=90}
<br>
<br>
```{r}
#| echo: true
#| code-line-numbers: "1|2"
#| output-location: fragment


lasso_tune |> 
  collect_metrics()
```

## Visualizar las Métricas

::: panel-tabset 

### autoplot

```{r}
#| echo: true
#| code-line-numbers: "1"
#| output-location: fragment
autoplot(lasso_tune)
```

### ggplot

```{r}
#| echo: true
#| code-line-numbers: "1|2|3|4|5|6"
#| output-location: fragment

lasso_tune |> 
  collect_metrics() |> 
  ggplot(aes(penalty, mean, color = .metric)) +
  geom_line(linewidth = 1.5) +
  facet_wrap(~.metric, scales = "free", nrow = 2) +
  theme(legend.position = "none")
```

:::

## Eligiendo nuestro parámetro final
![](https://tune.tidymodels.org/logo.png){.absolute top=0 right=0 width=80 height=90}
<br>
<br>
```{r}
#| echo: true
#| code-line-numbers: "1|2|4"
#| output-location: fragment

best <- lasso_tune |> 
  select_best(metric = "roc_auc")

best

```

## Finalizar el Flujo de Trabajo

```{r}
#| echo: true
#| code-line-numbers: "1|3"
#| output-location: fragment


final_wf <- finalize_workflow(texting_wf, best)

final_wf
```

## ¡Ajustemos el Modelo!

:::{.columns}
:::{.column width="70%"} 

```{r}
#| echo: true
#| code-line-numbers: "1|2|4"
#| output-location: fragment
texting_fit <- 
  fit(final_wf, data = analysis_train)

texting_fit

```

:::

:::{.column width="30%"} 
![](img/woddy_fit.gif)
:::
:::

## Revisar el ajuste en los datos de entrenamiento


```{r}
#| echo: true
#| code-line-numbers: "1|2|3|4|6"
#| output-location: fragment
texting_pred <- 
  augment(texting_fit, analysis_train) |> 
  select(TextingDriving, .pred_class, .pred_1, .pred_0)

texting_pred

```

## Gráfico ROC y valor AUC

:::{.columns}
:::{.column width="50%"} 

```{r}
#| echo: true
#| code-line-numbers: "1|2|3|4|5|7"
#| output-location: fragment

roc_plot_training <- 
  texting_pred |> 
  roc_curve(truth = TextingDriving, .pred_1, event_level = "second") |> 
  autoplot()

roc_plot_training 

```
:::


:::{.column width="50%"} 

```{r}
#| echo: true
#| code-line-numbers: "1|2"
#| output-location: fragment
texting_pred |>
    roc_auc(truth = TextingDriving, .pred_1, event_level = "second")

```

:::
:::

## Verificar las estimaciones 

```{r}
#| echo: true
#| code-line-numbers: "1|2|3"
#| output-location: fragment

texting_fit |> 
  extract_fit_parsnip() |> 
  tidy()

```


## Veamos las métricas en los remuestreos

```{r}
#| echo: true
#| code-line-numbers: "1|2|4"
#| output-location: fragment
texting_fit_resamples <- 
  fit_resamples(final_wf, resamples = analysis_folds)

texting_fit_resamples

```


## Explorando los resultados

```{r}
#| echo: true
#| code-line-numbers: "1|2|4"
#| output-location: fragment
collect_metrics(texting_fit_resamples)
```


## Ajustemos en los Datos de **PRUEBA** 


```{r}
#| echo: true
#| code-line-numbers: "1|2|3|4|5|6|8"
#| output-location: fragment

texting_last_fit <- 
  last_fit(final_wf,
           split = analysis_split,
           metrics = metric_set(accuracy, kap, roc_auc))

texting_last_fit

```


## Recolectar las Métricas
![](https://tune.tidymodels.org/logo.png){.absolute top=0 right=0 width=80 height=90}
<br>
<br>
```{r}
#| echo: true
#| code-line-numbers: "1"
#| output-location: fragment

collect_metrics(texting_last_fit)

```

## Revisar Predicciones en los Datos de Prueba con `yardstick`

```{r}
#| echo: true
#| code-line-numbers: "1|2|3|5"
#| output-location: fragment

the_prediction <-  
  texting_last_fit |> 
  collect_predictions() |> 
  select(-.config, -.row)
  

the_prediction 

```

## Revisar Predicciones en los Datos de Prueba con `yardstick`

```{r}
#| echo: true
#| code-line-numbers: "1|3|4|5|6"
#| output-location: fragment
multi_metric <- metric_set(sens, spec, accuracy, kap)

multi_metric(the_prediction, 
             truth = TextingDriving, 
             estimate = .pred_class, 
             event_level = "second")
```


## Matriz de Confusión en los Datos de Prueba
```{r}
the_prediction %>% 
  conf_mat(TextingDriving, .pred_class) |> 
  autoplot(type = "heatmap")
```

## Curva ROC en los Datos de Prueba
```{r}

the_prediction |> 
  roc_curve(truth = TextingDriving, .pred_1, event_level = "second") |> 
  autoplot()

```


## Gráfico de Importancia de Variables 

```{r}
#| echo: true
#| code-line-numbers: "1|3|4|4|5|6|7|8|9|10|11|12|13"
#| output-location: fragment
library(vip)

texting_last_fit |> 
  extract_fit_engine() |> 
  vi() |> 
  group_by(Sign) |> 
  slice_max(Importance, n = 5) |> 
  ungroup() |> 
  ggplot(aes(Importance, fct_reorder(Variable, Importance), fill = Sign)) + 
  geom_col() +
  facet_wrap(vars(Sign), scales = "free_y") +
  labs(y = NULL) +
  theme(legend.position = "none")
```

## ¡Lo logramos! 

![](https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExMTh3aWhnbWl5c3RkYm54OHlidjdsd2dsdnhtZDh0YzFrbjV2enBybiZlcD12MV9naWZzX3NlYXJjaCZjdD1n/nVXzt7FSJlX7W/giphy.gif)

## Preguntas de Revisión

::: {.fragment}
1. ¿Es la Suma Residual de Cuadrados un concepto relevante para las penalizaciones L1 y L2? ¿por qué?
:::

::: {.fragment}
2. ¿La regularización L2 sería mejor si la llamáramos regularización al cuadrado?
:::

::: {.fragment}
3. Si quiero reducir la dimensionalidad de un conjunto de datos usaría la penalización _______?
:::

::: {.fragment}
4. Si quiero que `tidymodels` realice una regresión ridge, ¿qué debo cambiar en este código?

```{r}
#| echo: true
review_spec <-
  logistic_reg(penalty = tune(), 
               mixture = tune()) |> 
  set_engine('glmnet')

`
``
:::

::: {.fragment}
5. BONUS: stepwise vs. lasso 
:::


