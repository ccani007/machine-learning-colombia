---
title: "Ãrboles de DecisiÃ³n usando `tidymodels`"
author: "Catalina CaÃ±izares, Ph.D. y Francisco Cardozo Ph.D."
format:
  revealjs:
    scrollable: true
    slide-number: true
    transition: "fade"
    auto-play-media: true
    auto-stretch: true
    css: style-jungle.css
    theme: default
    highlight-style: github
editor_options: 
  chunk_output_type: console
---
> Ãrboles de DecisiÃ³n usando `tidymodels`Â© 2024 por Catalina Canizares y Francisco Cardozo estÃ¡ licenciado bajo Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International 

Este material estÃ¡ disponible libremente bajo la Licencia Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International. 

Para mÃ¡s informaciÃ³n sobre esta licencia, por favor visite: [Licencia Creative Commons](https://creativecommons.org/licenses/by-nc-nd/4.0/)




## Agenda ğŸŒ²
```{r}
library(tidyverse)
library(tidymodels)
library(skimr)
library(MLearnYRBSS)
```

ğŸŒ² Ãrboles de DecisiÃ³n

ğŸŒ² Conceptos bÃ¡sicos (RaÃ­z, CaracterÃ­stica, Hoja)

ğŸŒ² Ver y entender las divisiones

ğŸŒ² EntropÃ­a y Ganancia de InformaciÃ³n

ğŸŒ² DetenciÃ³n temprana y Poda

ğŸŒ² Un Ã¡rbol en `tidymodels` 



## Ãrboles
:::: {.columns}
::: {.column width="50%"}

ğŸŒ² Los Ãrboles de DecisiÃ³n son algoritmos ampliamente utilizados para aprendizaje supervisado.

ğŸŒ² Proporcionan modelos interpretables para hacer predicciones tanto en tareas de regresiÃ³n como de clasificaciÃ³n.
:::
::: {.column width="50%"}

![](img/tree_example.png)

:::
:::


## CÃ³mo funciona 
ğŸŒ² Consiste en una serie de decisiones secuenciales sobre las caracterÃ­sticas de algÃºn conjunto de datos. 

![](img/1_tree.png)

:::footer
[Tune better models](https://tmv.netlify.app/site/slides/rmed03-tune.html#25)
:::


## CÃ³mo funciona 
![](img/2_tree.png)

:::footer
[Tune better models](https://tmv.netlify.app/site/slides/rmed03-tune.html#25)
:::

## CÃ³mo funciona 
![](img/3_tree.png)

:::footer
[Tune better models](https://tmv.netlify.app/site/slides/rmed03-tune.html#25)
:::

## CÃ³mo funciona 

![](img/4_tree.png)

:::footer
[Tune better models](https://tmv.netlify.app/site/slides/rmed03-tune.html#25)
:::

## CÃ³mo funciona 
![](img/5_tree.png)

:::footer
[Tune better models](https://tmv.netlify.app/site/slides/rmed03-tune.html#25)
:::

## CÃ³mo funciona 

![](img/6_tree.png)

:::footer
[Tune better models](https://tmv.netlify.app/site/slides/rmed03-tune.html#25)
:::

## CÃ³mo se ven las Divisiones

![](img/depth_1.png){fig-align="center"}

:::footer
[Tune better models](https://tmv.netlify.app/site/slides/rmed03-tune.html#25)
:::

## CÃ³mo se ven las Divisiones

![](img/depth_2.png){fig-align="center"}

:::footer
[Tune better models](https://tmv.netlify.app/site/slides/rmed03-tune.html#25)
:::

## CÃ³mo se ven las Divisiones

![](img/depth_3.png){fig-align="center"}

:::footer
[Tune better models](https://tmv.netlify.app/site/slides/rmed03-tune.html#25)
:::

## CÃ³mo se ven las Divisiones
![](img/depth_7.png){fig-align="center"}

:::footer
[Tune better models](https://tmv.netlify.app/site/slides/rmed03-tune.html#25)
:::

## CÃ³mo se ven las Divisiones
![](img/depth_8.png){fig-align="center"}

:::footer
[Tune better models](https://tmv.netlify.app/site/slides/rmed03-tune.html#25)
:::

## CÃ³mo se ven las Divisiones
![](img/depth_9.png){fig-align="center"}

:::footer
[Tune better models](https://tmv.netlify.app/site/slides/rmed03-tune.html#25)
:::

## CÃ³mo se ven las Divisiones
![](img/depth_10.png)


:::footer
[Tune better models](https://tmv.netlify.app/site/slides/rmed03-tune.html#25)
:::

## Â¿CÃ³mo determina el algoritmo dÃ³nde particionar los datos?  

ğŸŒ² En lugar de minimizar la Suma de Errores al Cuadrado, puedes minimizar la entropÃ­a....

ğŸŒ² **EntropÃ­a** = mide la cantidad de informaciÃ³n de alguna variable o evento.

ğŸŒ² La usaremos para identificar regiones que consisten en  

   - Un gran nÃºmero de elementos similares (puros) o 
   
   - Elementos disÃ­miles (impuros).


## Ganancia de InformaciÃ³n - La lÃ³gica para entrenar  

ğŸŒ² Mide la calidad de una divisiÃ³n

ğŸŒ² El algoritmo central para calcular la ganancia de informaciÃ³n se llama ID3.

ğŸŒ² Se calcula para una divisiÃ³n restando las entropÃ­as ponderadas de cada rama de la entropÃ­a original. 
ğŸŒ² Al entrenar un Ãrbol de DecisiÃ³n usando estas mÃ©tricas, la mejor divisiÃ³n se elige maximizando la Ganancia de InformaciÃ³n.

ğŸŒ² Selecciona la divisiÃ³n que produce la mayor reducciÃ³n en entropÃ­a, o, el mayor aumento en informaciÃ³n.


## Ganancia de InformaciÃ³n
[Haz clic aquÃ­ para ver la animaciÃ³n](https://mlu-explain.github.io/decision-tree/)

```{=html}
<iframe width="1000" height="700" src="https://mlu-explain.github.io/decision-tree/" title="Webpage example"></iframe>
```


## Ganancia de InformaciÃ³n 

Si estÃ¡s interesado en las matemÃ¡ticas: 
[Una ExplicaciÃ³n Simple de Ganancia de InformaciÃ³n y EntropÃ­a](https://victorzhou.com/blog/information-gain/) 

## Ãrboles de clasificaciÃ³n

ğŸŒ² Una de las preguntas que surge en un algoritmo de Ã¡rbol de decisiÃ³n es: **Â¿cuÃ¡l es el tamaÃ±o Ã³ptimo del Ã¡rbol final?**

ğŸŒ² Un Ã¡rbol que es **demasiado grande** corre el riesgo de sobreajustar los datos de entrenamiento y generalizar mal a nuevas muestras.

ğŸŒ² Un **Ã¡rbol pequeÃ±o** podrÃ­a no capturar informaciÃ³n estructural importante sobre el espacio de muestras.

ğŸŒ² Sin embargo, **es difÃ­cil saber** cuÃ¡ndo un algoritmo de Ã¡rbol debe detenerse!

# DetenciÃ³n Temprana

## tree_depth

ğŸŒ² Limita la profundidad mÃ¡xima del Ã¡rbol.

ğŸŒ² Un mÃ©todo para detener el Ã¡rbol tempranamente.

ğŸŒ² Se usa para prevenir el sobreajuste.

## tree_depth  

![](img/tree_depth1.png)

:::footer
[Tune better models](https://tmv.netlify.app/site/slides/rmed03-tune.html#25)
:::

## tree_depth 

![](img/tree_depth2.png)


:::footer
[Tune better models](https://tmv.netlify.app/site/slides/rmed03-tune.html#25)
:::

## min_n

ğŸŒ² Un entero para el nÃºmero mÃ­nimo de puntos de datos en un nodo que se requieren para que el nodo se divida mÃ¡s.

ğŸŒ² Establece el n mÃ­nimo para dividir en cualquier nodo.

ğŸŒ² Otro mÃ©todo de detenciÃ³n temprana.

ğŸŒ² Se usa para prevenir el sobreajuste.

ğŸŒ² min_n = 1 llevarÃ­a al Ã¡rbol mÃ¡s sobreajustado.

:::footer
[Tune better models](https://tmv.netlify.app/site/slides/rmed03-tune.html#25)
:::

# Poda 

## cost_complexity - poda del Ã¡rbol

ğŸŒ² AÃ±ade un costo o penalizaciÃ³n a las tasas de error de Ã¡rboles mÃ¡s complejos
 
ğŸŒ² Se usa para prevenir el sobreajuste.

ğŸŒ² MÃ¡s cercano a cero â¡ï¸ Ã¡rboles mÃ¡s grandes.

ğŸŒ² PenalizaciÃ³n mÃ¡s alta â¡ï¸ Ã¡rboles mÃ¡s pequeÃ±os.

:::footer
[Tune better models](https://tmv.netlify.app/site/slides/rmed03-tune.html#25)
:::



## cost_complexity

$$
R_\alpha(T) = R(T) + \alpha|\widetilde{T}|
$$
ğŸŒ² $R(T)$ tasa de clasificaciÃ³n errÃ³nea 

ğŸŒ² Para cualquier subÃ¡rbol $T<T_{max}$ definiremos su complejidad como  $|\widetilde{T}|$

ğŸŒ² $|\widetilde{T}|$ = el nÃºmero de nodos terminales u hojas en T. 

ğŸŒ² $\alpha â‰¤0$ sea un nÃºmero real llamado el parÃ¡metro de complejidad. 

ğŸŒ² Si $\alpha$ = 0 entonces se elegirÃ¡ el Ã¡rbol mÃ¡s grande porque el tÃ©rmino de penalizaciÃ³n por complejidad esencialmente se elimina.

ğŸŒ² A medida que $\alpha$ se acerca al infinito, se seleccionarÃ¡ el Ã¡rbol de tamaÃ±o 1.

:::footer
[Applied Data Mining and Statistical Learning](https://online.stat.psu.edu/stat508/lesson/11/11.8/11.8.2)
:::

## cost_complexity

![](img/cost_complex1.png)

:::footer
[Tune better models](https://tmv.netlify.app/site/slides/rmed03-tune.html#25)
:::

## cost_complexity

![](img/cost_complex2.png)

:::footer
[Tune better models](https://tmv.netlify.app/site/slides/rmed03-tune.html#25)
:::

## Resumen 

![](img/recap_cp.png)

:::footer
[Tune better models](https://tmv.netlify.app/site/slides/rmed03-tune.html#25)
:::

# Ãrbol de ClasificaciÃ³n con `tidymodels`   

![](https://media.giphy.com/media/F9hQLAVhWnL56/giphy.gif){fig-align="center"}



## Tarea

Predecir si un adolescente ha consumido alcohol o no basÃ¡ndose en un conjunto de varios comportamientos de riesgo. 

![](https://media.giphy.com/media/JzujPK0id34qI/giphy.gif){fig-align="center"}


## Limpieza de Datos

```{r}
#| echo: true
#| code-line-numbers: "1|3|4|5|6|7|9|10|11|12|13"

data("riskyBehaviors")

riskyBehaviors_analysis <-
    riskyBehaviors |>
    mutate(
        UsedAlcohol = case_when(
            AgeFirstAlcohol == 1 ~ 0,
            AgeFirstAlcohol %in% c(2, 3, 5, 6, 4, 7) ~ 1,
            TRUE ~ NA
        )
    ) |>
    mutate(UsedAlcohol = factor(UsedAlcohol)) |>
    drop_na(UsedAlcohol) |>
    select(
        -c(
            AgeFirstAlcohol,
            DaysAlcohol,
            BingeDrinking,
            LargestNumberOfDrinks,
            SourceAlcohol,
            SourceAlcohol
        )
    )

```


## DivisiÃ³n de los datos 

```{r}
#| echo: true
#| code-line-numbers: "1|3|4|6|7|9"
#| output-location: fragment

set.seed(2023)

alcohol_split <- initial_split(riskyBehaviors_analysis, strata = UsedAlcohol)

alcohol_train <- training(alcohol_split)
alcohol_test <- testing(alcohol_split)

alcohol_split
```


## Verifiquemos Nuestro Trabajo

```{r}
library(janitor)
```


:::: {.columns}
::: {.column width="50%"}

```{r}
#| echo: true
#| code-line-numbers: "1|2|3|4"
#| output-location: fragment

alcohol_train |>
    tabyl(UsedAlcohol) |>
    adorn_pct_formatting(0) |>
    adorn_totals()


```

:::

::: {.column width="50%"}

```{r}
#| echo: true
#| code-line-numbers: "1|2|3|4"
#| output-location: fragment

alcohol_test |>
    tabyl(UsedAlcohol) |>
    adorn_pct_formatting(0) |>
    adorn_totals()
```

:::
:::

## Creando el Objeto de Remuestreo 

```{r}
#| echo: true
#| code-line-numbers: "1|3|4|5"
#| output-location: fragment

set.seed(2023)

cv_alcohol <- rsample::vfold_cv(alcohol_train, strata = UsedAlcohol)
cv_alcohol
```



## La Receta

```{r}
#| echo: true
#| code-line-numbers: "1|2|3|4"
#| output-location: fragment

alcohol_recipe <-
    recipe(formula = UsedAlcohol ~ ., data = alcohol_train) |>
    step_impute_mode(all_nominal_predictors()) |>
    step_impute_mean(all_numeric_predictors())


```

## La EspecificaciÃ³n

```{r}
#| echo: true
#| code-line-numbers: "1|2|3|4|5|6|7|9"
#| output-location: fragment

cart_spec <-
    decision_tree(
        cost_complexity = tune(),
        tree_depth = tune(),
        min_n = tune()
    ) |>
    set_engine("rpart") |>
    set_mode("classification")

cart_spec
```


## El Flujo de Trabajo

```{r}
#| echo: true
#| code-line-numbers: "1|2|3|4|6"
#| output-location: fragment

cart_workflow <-
    workflow() |>
    add_recipe(alcohol_recipe) |>
    add_model(cart_spec)

cart_workflow
```

## Ajuste del Ã¡rbol - La CuadrÃ­cula

```{r}
#| echo: true
#| code-line-numbers: "1|2|3|4|5|6"
#| output-location: fragment

tree_grid <-
    grid_regular(cost_complexity(), tree_depth(c(2, 5)), min_n(), levels = 4)
tree_grid

```

## Ajuste del Ã¡rbol

```{r}
#| echo: true
#| eval: false
#| code-line-numbers: "1|3|5|6|7|8|10"
#| output-location: fragment

doParallel::registerDoParallel()

cart_tune <-
    cart_workflow %>%
    tune_grid(
        resamples = cv_alcohol,
        grid = tree_grid,
        metrics = metric_set(roc_auc),
        control = control_grid(save_pred = TRUE)
    )

doParallel::stopImplicitCluster()
```


```{r}
#| echo: false

# saveRDS(cart_tune, "outputs/cart_tune.rds")
cart_tune <- readRDS("outputs/cart_tune.rds")
```

## Eligiendo el mejor CP

```{r}
#| echo: true
#| code-line-numbers: "1"
#| output-location: fragment

show_best(cart_tune, metric = "roc_auc")
```

## Eligiendo los mejores hiperparÃ¡metros

```{r}
#| echo: true
#| code-line-numbers: "1|2|3|5"
#| output-location: fragment

bestPlot_cart <-
    autoplot(cart_tune)

bestPlot_cart
```

## Eligiendo el mejor CP

```{r}
#| echo: true
#| code-line-numbers: "1|2|3|4"
#| output-location: fragment

best_cart <- select_best(
    cart_tune,
    metric = "roc_auc"
)

best_cart
```

## Finalizando el Flujo de Trabajo

```{r}
#| echo: true
#| code-line-numbers: "1|2"
#| output-location: fragment

cart_final_wf <- finalize_workflow(cart_workflow, best_cart)
cart_final_wf
```

## Ajustar el Ã¡rbol

:::: {.columns}
::: {.column width="60%"}


```{r}
#| echo: true
#| code-line-numbers: "1|2|3|5"
#| output-location: fragment

cart_fit <- fit(
    cart_final_wf,
    alcohol_train
)

cart_fit
```

:::

::: {.column width="40%"}


![](https://media.giphy.com/media/Cv1j2hKet0OamdSVFg/giphy.gif)
:::
:::

## Revisar el ajuste en los datos de entrenamiento

```{r}
#| echo: true
#| code-line-numbers: "1|2|3|5"
#| output-location: fragment

tree_pred <-
    augment(cart_fit, alcohol_train) |>
    select(UsedAlcohol, .pred_class, .pred_1, .pred_0)

tree_pred

```

## Revisar el ajuste en los datos de entrenamiento

:::: {.columns}
::: {.column width="50%"}
```{r}
#| echo: true
#| code-line-numbers: "1|2|3|4|5|6|8"
#| output-location: fragment

roc_tree <-
    tree_pred |>
    roc_curve(truth = UsedAlcohol, .pred_1, event_level = "second") |>
    autoplot()

roc_tree

```

:::

::: {.column width="50%"}
```{r}
#| echo: true
#| code-line-numbers: "1|2|3|4"
#| output-location: fragment

tree_pred |> 
  roc_auc(truth = UsedAlcohol, 
           .pred_1, 
           event_level = "second")

```

:::
:::

## RevisiÃ³n en Remuestras

```{r}
#| echo: true
#| code-line-numbers: "1|2|3"
#| output-location: fragment
fit_resamples(cart_final_wf, resamples = cv_alcohol) |>
    collect_metrics()
```


## El Ã¡rbol
```{r}
#| echo: true
#| code-line-numbers: "1|2|3"
#| output-location: fragment

cart_fit |>
    extract_fit_engine() |>
    rpart.plot::rpart.plot(roundint = FALSE)
```

## ContinuarÃ¡... 

:::: {.columns}
::: {.column width="60%"} 

Este modelo aÃºn no ha sido probado, ya que planeamos realizar un anÃ¡lisis adicional. En la prÃ³xima presentaciÃ³n, utilizaremos los mismos datos de entrenamiento con el algoritmo Random Forest, seguido de la evaluaciÃ³n de su rendimiento usando el conjunto de prueba.
:::
::: {.column width="40%"} 

![](https://media.giphy.com/media/3o7budMRwZvNGJ3pyE/giphy.gif)
:::
:::
